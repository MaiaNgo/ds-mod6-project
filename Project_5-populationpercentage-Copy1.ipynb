{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mod 5 Project - Using Machine Learning to Predict Coronavirus\n",
    "\n",
    "This project uses daily updated coronavirus data from John Hopkins Hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID-19 MAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HOW MANY COUNTRIES, HOW MANY PEOPLE CONTAGIOUS, HOW MANY DEATH CASES???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCENTAGE OF CONFIRMED CASES/POPULATION, DEATH CASES/CONFIRMED CASES, RECOVERED/CONFIRMED CASES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: IMPORT ALL NECCESSARY LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all neccessary libraries\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pylab import rcParams\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.eval_measures import rmse\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.graphics.tsaplots import plot_acf,plot_pacf # for determining (p,q) orders\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose      # for ETS Plots\n",
    "from pmdarima import auto_arima                              # for determining ARIMA orders\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# ignore all harmless waring to keep the notebook clean\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# keep the plot inline in notebookb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "import pyarrow\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "#set ggplot style\n",
    "plt.style.use('ggplot')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trend_2020-03-26.csv\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Passed non-file path: ./data/2020-03-26/agg_data_2020-03-26.parquet.gzip",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1df41d2e943f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0magg_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdaily_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'use_pandas_metadata'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         result = self.api.parquet.read_table(path, columns=columns,\n\u001b[0;32m--> 129\u001b[0;31m                                              **kwargs).to_pandas()\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                             \u001b[0mread_dictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_dictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                             \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m                             filesystem=filesystem, filters=filters)\n\u001b[0m\u001b[1;32m   1275\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m         pf = ParquetFile(source, metadata=metadata,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size)\u001b[0m\n\u001b[1;32m   1028\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_manifest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m              \u001b[0mpath_or_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata_nthreads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m              \u001b[0mopen_file_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_dataset_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         )\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m_make_manifest\u001b[0;34m(path_or_paths, fs, pathsep, metadata_nthreads, open_file_func)\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 raise IOError('Passed non-file path: {0}'\n\u001b[0;32m-> 1229\u001b[0;31m                               .format(path))\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0mpiece\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetDatasetPiece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen_file_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen_file_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mpieces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpiece\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Passed non-file path: ./data/2020-03-26/agg_data_2020-03-26.parquet.gzip"
     ]
    }
   ],
   "source": [
    "# Dynamic parameters\n",
    "data_dir  = './data/' + str(datetime.date(datetime.now()))\n",
    "agg_file  = 'agg_data_{}.parquet.gzip'.format(datetime.date(datetime.now()))\n",
    "trend_file  = 'trend_{}.csv'.format(datetime.date(datetime.now()))\n",
    "report  = 'report_{}.xlsx'.format(datetime.date(datetime.now()))\n",
    "\n",
    "COUNTRY = 'Uk'\n",
    "\n",
    "\n",
    "print(trend_file)\n",
    "\n",
    "# import data\n",
    "agg_df = pd.read_parquet(os.path.join(data_dir, agg_file))\n",
    "daily_df = pd.read_csv(os.path.join(data_dir, trend_file))\n",
    "\n",
    "# daily_df.new_confirmed_cases = daily_df.new_confirmed_cases.abs()\n",
    "\n",
    "#Create place to save diagrams\n",
    "image_dir = './images/'\n",
    "if not os.path.exists(image_dir):\n",
    "    os.mkdir(image_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: LOAD DATA\n",
    "#### We have 3 data sets: Confirmed cases, Death cases & Recovered cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "url = 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "df_confirmed = pd.read_csv(url + 'time_series_19-covid-Confirmed.csv')\n",
    "df_deaths = pd.read_csv(url + 'time_series_19-covid-Deaths.csv')\n",
    "df_recovered = pd.read_csv(url + 'time_series_19-covid-Recovered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_K = pd.read_csv('population_by_country_2020.csv')\n",
    "df_population_K.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_population_K.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020_K = df_population_K.filter(['Country (or dependency)', 'Population (2020)'])\n",
    "df_2020_K.reset_index(inplace=True)\n",
    "df_2020_K.drop(['index'], axis=1, inplace=True)\n",
    "df_2020_K.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: INITIAL EXPLORATORY DATA ANALYSIS - EDA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/ INITIAL ANALYSIS - df_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count the null columns\n",
    "null_columns = df_confirmed.columns[df_confirmed.isnull().any()]\n",
    "df_confirmed[null_columns].isnull().sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_columns', 999)\n",
    "print(df_confirmed[df_confirmed.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_rows', 999)\n",
    "print(df_confirmed[df_confirmed.isnull().any(axis=1)][null_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_confirmed['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_confirmed['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_confirmed['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_confirmed['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B/ INITIAL ANALYSIS - df_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count the null columns\n",
    "null_columns = df_deaths.columns[df_deaths.isnull().any()]\n",
    "df_deaths[null_columns].isnull().sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_columns', 999)\n",
    "print(df_deaths[df_deaths.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_rows', 999)\n",
    "print(df_deaths[df_deaths.isnull().any(axis=1)][null_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_deaths['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_deaths['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_deaths['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C/ INITIAL ANALYSIS - df_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count the null columns\n",
    "null_columns = df_recovered.columns[df_recovered.isnull().any()]\n",
    "df_recovered[null_columns].isnull().sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_columns', 999)\n",
    "print(df_recovered[df_recovered.isnull().any(axis=1)][null_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glance look at the null data\n",
    "pd.set_option('display.max_rows', 999)\n",
    "print(df_recovered[df_recovered.isnull().any(axis=1)][null_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_recovered['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_recovered['Province/State'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_recovered['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_recovered['Country/Region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_list = list(df_recovered['Country/Region'].unique())\n",
    "covid19_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_population_K = df_2020_K[df_2020_K['Country (or dependency)'].isin(covid19_list)]\n",
    "covid19_population_K.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_population_K.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 4: CLEANING DATA, REDUCE UNNECCESSARY & MISSING DATA FEATURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The column 'Province/State' has 143 missing data, and we actually focus on country information not detail to Province/State. We are going reduce the Province/State column and using total cases by country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that we don't need them\n",
    "# we focus on data by Country/Region only\n",
    "dropped_df_confirmed = df_confirmed.drop(['Province/State','Lat', 'Long'], axis=1)\n",
    "dropped_df_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed_total = dropped_df_confirmed.groupby(['Country/Region']).sum()\n",
    "dropped_df_confirmed_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed_total.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropped_df_confirmed_total.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dropped_df_confirmed_total[\"current_cases\"] = dropped_df_confirmed_total.iloc[:, -1]\n",
    "dropped_df_confirmed_total = dropped_df_confirmed_total[ ['current_cases'] + [ col for col in dropped_df_confirmed_total.columns if col != 'current_cases' ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_confirmed=dropped_df_confirmed_total.drop(['current_cases'], axis=1)\n",
    "sorted_df_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_confirmed_total.sort_values(by='current_cases', ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_confirmed = dropped_df_confirmed_total.iloc[0:30,:]\n",
    "top30_confirmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_confirmed.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_confirmed_list = top30_confirmed['Country/Region']\n",
    "top_confirmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_deaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that we don't need them\n",
    "# we focus on data by Country/Region only\n",
    "dropped_df_deaths = df_deaths.drop(['Province/State','Lat', 'Long'], axis=1)\n",
    "dropped_df_deaths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths.describe(include=['object', 'bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths_total = dropped_df_deaths.groupby(['Country/Region']).sum()\n",
    "dropped_df_deaths_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths_total.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dropped_df_deaths_total[\"current_deaths\"] = dropped_df_deaths_total.iloc[:, -1]\n",
    "dropped_df_deaths_total = dropped_df_deaths_total[ ['current_deaths'] + [ col for col in dropped_df_deaths_total.columns if col != 'current_deaths' ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_deaths=dropped_df_deaths_total.drop(['current_deaths'], axis=1)\n",
    "sorted_df_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths_total.sort_values(by='current_deaths', ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_deaths = dropped_df_deaths_total.iloc[0:30,:]\n",
    "top30_deaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_deaths.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_deaths_list = top30_deaths['Country/Region']\n",
    "top_deaths_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df9_population = df_2020_K[df_2020_K['Country (or dependency)'].isin(top_deaths_list)]\n",
    "# df9_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped_df_deaths_total.sort_values(by='current_deaths', ascending=False, inplace = True)\n",
    "\n",
    "# top30_deaths = dropped_df_deaths_total.iloc[0:30,:]\n",
    "# top30_deaths\n",
    "\n",
    "# top30_deaths.reset_index(inplace = True)\n",
    "\n",
    "# sns.set_style('whitegrid')\n",
    "# g = sns.catplot(x='Country/Region', y='current_deaths', data=top30_deaths,\n",
    "#                 kind='bar', palette='cool', height=6, aspect=2.5, legend = False)\n",
    "# plt.legend(loc='upper right');\n",
    "# g.set_xticklabels(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_deaths_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns that we don't need them\n",
    "# we focus on data by Country/Region only\n",
    "dropped_df_recovered = df_recovered.drop(['Province/State','Lat', 'Long'], axis=1)\n",
    "dropped_df_recovered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_recovered_total = dropped_df_recovered.groupby(['Country/Region']).sum()\n",
    "dropped_df_recovered_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_recovered_total.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_recovered_total.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_recovered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dropped_df_recovered_total[\"current_recovered\"] = dropped_df_recovered_total.iloc[:, -1]\n",
    "dropped_df_recovered_total = dropped_df_recovered_total[ ['current_recovered'] + [ col for col in dropped_df_recovered_total.columns if col != 'current_recovered' ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_recovered=dropped_df_recovered_total.drop(['current_recovered'], axis=1)\n",
    "sorted_df_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_df_recovered_total.sort_values(by='current_recovered', ascending=False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_recovered = dropped_df_recovered_total.iloc[0:30,:]\n",
    "top30_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top30_recovered.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_recovered_list = top30_recovered['Country/Region']\n",
    "top_recovered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dropped_df_confirmed_total.filter(['current_cases'])\n",
    "df1.reset_index(inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dropped_df_deaths_total.filter(['current_deaths'])\n",
    "df2.reset_index(inplace=True)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = dropped_df_recovered_total.filter(['current_recovered'])\n",
    "df3.reset_index(inplace=True)\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_population_K.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_population_K.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid19_population_K.rename(columns = {'Country (or dependency)':'Country/Region'}, inplace = True) \n",
    "covid19_population_K.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.merge(df1, df2, on=['Country/Region'])\n",
    "df5 = pd.merge(df3, df4, on=['Country/Region'])\n",
    "df5_pop = pd.merge(df5, covid19_population_K, on=['Country/Region'])\n",
    "df5_pop['Confirmed Percentage'] = (df5_pop['current_cases'] / df5_pop['Population (2020)']) *100\n",
    "df5_pop['Recovered Percentage'] = (df5_pop['current_recovered'] / df5_pop['Population (2020)']) *100\n",
    "df5_pop['Deaths Percentage'] = (df5_pop['current_deaths'] / df5_pop['Population (2020)']) *100\n",
    "\n",
    "#df5.head()\n",
    "df5_pop.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_per = df5_pop.filter(['Country/Region','Confirmed Percentage', 'Recovered Percentage', 'Deaths Percentage'])\n",
    "df5_per.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.sort_values(['current_cases', 'current_deaths', 'current_recovered'], ascending=False)\n",
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING PERCENTAGE OF CASES PER POPULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = pd.melt(df5_per, id_vars=['Country/Region'], value_vars=['Confirmed Percentage', 'Deaths Percentage', 'Recovered Percentage'], \n",
    "              var_name='Types', value_name='Number Of Cases')\n",
    "df7.info()\n",
    "df7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df8 = df7[df7['Country/Region'].isin(top_confirmed_list) ]\n",
    "df8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = df7[df7['Country/Region'].isin(top_deaths_list) ]\n",
    "df9.head()\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "g = sns.catplot(data=df9, x='Country/Region', y='Number Of Cases',\n",
    "                           hue='Types', kind=\"bar\", palette='cool', height=5, aspect = 2.5, legend = False)\n",
    "plt.title('TOP 30 COUNTRIES OF DEATHS CASES ', size=16)\n",
    "plt.legend(loc='upper right');\n",
    "g.set_xticklabels(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df8_population = df_2020[df_2020['Location'].isin(top_confirmed_list)]\n",
    "# df8_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "g = sns.catplot(data=df8, x='Country/Region', y='Number Of Cases',\n",
    "                           hue='Types', kind=\"bar\", palette='hot', height=5, aspect = 2.5, legend = False)\n",
    "plt.title('TOP 30 COUNTRIES OF CONFIRMED CASES ', size=16)\n",
    "plt.legend(loc='upper right');\n",
    "g.set_xticklabels(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df10_population = df_2020[df_2020['Location'].isin(top_recovered_list)]\n",
    "# df10_population.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df10 = df7[df7['Country/Region'].isin(top_recovered_list) ]\n",
    "df10.head()\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "g = sns.catplot(data=df10, x='Country/Region', y='Number Of Cases',\n",
    "                           hue='Types', kind=\"bar\", palette='autumn', height=5, aspect = 2.5, legend = False)\n",
    "plt.title('TOP 30 COUNTRIES OF RECOVERED CASES ', size=16)\n",
    "plt.legend(loc='upper right');\n",
    "g.set_xticklabels(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top30_recovered.reset_index(inplace = True)\n",
    "# sns.set_style('whitegrid')\n",
    "# g = sns.catplot(x='Country/Region', y='current_recovered', data=top30_recovered,\n",
    "#                 kind='bar', palette='autumn', height=6, aspect=2.5, legend = False)\n",
    "# plt.legend(loc='upper right');\n",
    "# g.set_xticklabels(rotation=45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 5: RESHAPE FROM WIDE TO LONG FORMAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_data(df):\n",
    "    \"\"\"\n",
    "    melt data of one zip code from wide format to long format\n",
    "    \"\"\"\n",
    "    \n",
    "    melted = pd.melt(df, id_vars=['Country/Region'], var_name='time')\n",
    "    melted['time'] = pd.to_datetime(melted['time'], infer_datetime_format=True)\n",
    "    #melted = melted.dropna(subset=['value'])\n",
    "    \n",
    "    return melted.groupby('time').aggregate({'value':'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df(df):\n",
    "    \"\"\"\n",
    "    Loop through all zipcodes to melt data of each zipcode.\n",
    "    Then, merge all melted data back together\n",
    "    \"\"\"\n",
    "    \n",
    "    merged = []\n",
    "    for country in df['Country/Region']:\n",
    "        melted = melt_data(df.loc[df['Country/Region'] == country])\n",
    "        row = df.loc[df['Country/Region'] == country].iloc[:,:1]\n",
    "        rows = pd.concat([row]*len(melted), ignore_index=True)\n",
    "        merge = pd.concat([rows, melted.reset_index()], axis= 1)\n",
    "        merged.append(merge)\n",
    "    melted_df = pd.concat(merged)\n",
    "    return melted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_confirmed.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = melt_df(sorted_df_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any columns has na/nan value, if there is missing data\n",
    "#### forward fill missing value\n",
    "#df['time'] = df['time'].ffill()\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Time Series Data by setting the time column as the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it as time series\n",
    "df.set_index('time', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the head again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check any columns has na/nan value, if there is missing data\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we see no more missing value in any column. Change the name of feature. Take a look at the final dataframe before performing EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "df.rename(columns={'Country/Region': 'Country'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: EDA and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of data at a glance, just to see the whole picture of corona virus spreading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countries affected\n",
    "countries = df['Country'].unique()\n",
    "len(countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "for c in countries:\n",
    "    df[df['Country']==c]['value'].plot(label=c)\n",
    "\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), fancybox=True, shadow=True, ncol=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From a quick plot above, we see that the corona virus had started and spreaded widely from China since Jan 22, 2020. There are some countries had started slightly from Feb 20 and the corona virus were actually growing much faster over time. Jan 26, 2020 the spreading speed has increased significantly in some countries such as Singapore, Spain, Iran, Italy . Mar 10, 2020 the corona virus has been spreading out crazily in other countries while there is no new confirmed case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run an ETS Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12,5\n",
    "\n",
    "for country in top_confirmed_list[0:10]:\n",
    "    results = seasonal_decompose(df.loc[df['Country'] == country].value, model='add')\n",
    "    fig = results.plot();\n",
    "    fig.text(0.5, 1, f'COUNTRY: {country}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot ACF and PACF of some zipcodes to check corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,40))\n",
    "i = 0\n",
    "lags=40\n",
    "for country in top_confirmed_list[0:10]:\n",
    "    i += 1\n",
    "    ax = plt.subplot(10,2,i)\n",
    "    title = f'Autocorrelation: Country: {country}'\n",
    "    plot_acf(df.loc[df['Country'] == country].value, alpha=0.05, title=title, lags=lags, ax=ax);\n",
    "\n",
    "    i += 1\n",
    "    ax = plt.subplot(10,2,i)\n",
    "    title=f'Partial Autocorrelation: Country: {country}'\n",
    "    plot_pacf(df.loc[df['Country'] == country].value, alpha=0.05, title=title, lags=lags, ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the Augmented Dickey-Fuller Test\n",
    "##### Function that performs the augmented Dickey-Fuller Test to determine if an incoming time series is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def adf_test(series,title=''):\n",
    "    \"\"\"\n",
    "    Pass in a time series and an optional title, returns an ADF report\n",
    "    \"\"\"\n",
    "    print(\"==============================================================\")\n",
    "    print(f'Augmented Dickey-Fuller Test: {title}')\n",
    "    result = adfuller(series.dropna(),autolag='AIC') # .dropna() handles differenced data\n",
    "    \n",
    "    labels = ['ADF test statistic','p-value','# lags used','# observations']\n",
    "    out = pd.Series(result[0:4],index=labels)\n",
    "\n",
    "    for key,val in result[4].items():\n",
    "        out[f'critical value ({key})']=val\n",
    "        \n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(out.to_string())\n",
    "    \n",
    "    if result[1] <= 0.05:\n",
    "        print(\"Strong evidence against the null hypothesis\")\n",
    "        print(\"Reject the null hypothesis\")\n",
    "        print(\"Data has no unit root and is stationary\")\n",
    "    else:\n",
    "        print(\"Weak evidence against the null hypothesis\")\n",
    "        print(\"Fail to reject the null hypothesis\")\n",
    "        print(\"Data has a unit root and is non-stationary\")\n",
    "        \n",
    "    print(\"--------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check ADF test for some zipcodes\n",
    "for country in top_confirmed_list[0:10]:\n",
    "    adf_test(df.loc[df['Country'] == country].value, title= f'COUNTRY: {country}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the Augmented Dickey-Fuller Test results, all data for all zipcodes are non-staytionary. except Italy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 8: Explore ARIMA Modeling to Find Best Fit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use grid search to find best fit ARIMA model of one sample country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One sample test zipcode\n",
    "country = top_confirmed_list[0]\n",
    "country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with one zipcode\n",
    "result = auto_arima(df.loc[df['Country'] == country].value, \n",
    "                    start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=False, d=None, D=1, \n",
    "                    trace=False, error_action='ignore',suppress_warnings=True, stepwise=True)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After having the best-fit model from auto_arima grid search, we double check with visualization of ETS decomposition and ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 12,5\n",
    "results = seasonal_decompose(df.loc[df['Country'] == country].value, model='add')\n",
    "fig = results.plot();\n",
    "fig.text(0.5,1,f'COUNTRY: {country}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(df.loc[df['Country'] == country].value,alpha=0.05,title='ACF',lags=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df.loc[df['Country'] == country].value,alpha=0.05,title='PACF',lags=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, the ACF & PACF plots confirm the zipcode has none seasonality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we wrote code to find best-fit models for a list of countries and store the fitted models in to a DataFrame for later dispaly and use\n",
    "#### For quick testing purpose we only need to test with one country, but we can do for all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best fit model for the first test country\n",
    "models = []\n",
    "for country in top_confirmed_list[0:1]:\n",
    "    result = auto_arima(df.loc[df['Country'] == country].value, \n",
    "                    start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=False, d=None, D=1, \n",
    "                    trace=False, error_action='ignore',suppress_warnings=True, stepwise=True)\n",
    "    if result._is_seasonal():\n",
    "        model = {'Country': country, 'model': 'SARIMAX', 'order': result.order,\n",
    "                                     'seasonal_order': result.seasonal_order}\n",
    "    else:\n",
    "        model = {'Country': country, 'model': 'ARIMA', 'order': result.order, \n",
    "                                     'seasonal_order': None}\n",
    "        \n",
    "    models.append(model)\n",
    "    \n",
    "# convert models list into DataFrame for easy reading\n",
    "model_df = pd.DataFrame(models, columns=['zipcode', 'model', 'order', 'seasonal_order'])\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After having the model, we split data of this country into train/test dataset to evaluate the accurateness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_country_data = df[df['Country']==country]\n",
    "first_country_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tail data of 3 year - test data set\n",
    "first_country_data.tail(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the split time point for train & test data.\n",
    "now = datetime.now()\n",
    "start_date = datetime(2020, 1, 22)\n",
    "end_date = now\n",
    "delta = end_date - start_date\n",
    "train_days = round(delta.days * 0.7)\n",
    "split_date = start_date + timedelta(days=train_days)\n",
    "\n",
    "start_date = start_date.date()\n",
    "end_date = end_date.date()\n",
    "split_date = split_date.date()\n",
    "\n",
    "print(start_date)\n",
    "print(split_date)\n",
    "print(end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test data\n",
    "#train_df = first_country_data.iloc[:train_days] #\n",
    "train_df = first_country_data.loc[:split_date]\n",
    "#test_df = first_country_data.iloc[split_date:]  #\n",
    "test_start_date = split_date + timedelta(days=1) \n",
    "test_df = first_country_data.loc[test_start_date:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best model with full historical data for one test zipcode\n",
    "arima_model = sm.tsa.statespace.SARIMAX(train_df['value'], \n",
    "                                        enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "# fit the model and print results\n",
    "fitted_model = arima_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the diagnostics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model diagnostics\n",
    "fitted_model.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the prediction and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast the test data\n",
    "forecast_values = fitted_model.predict(start=test_start_date, end=end_date, \n",
    "                                       typ='levels', dynamic=True).rename('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical and forecasted data\n",
    "train_df['train'] = train_df['value']\n",
    "train_df['train'].plot(legend=True,figsize=(12,6))\n",
    "test_df['test'] = test_df['value']\n",
    "test_df['test'].plot(legend=True)\n",
    "forecast_values.plot(legend=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best fit model for the first test country\n",
    "models = []\n",
    "for country in top_confirmed_list[1:2]:\n",
    "    result = auto_arima(df.loc[df['Country'] == country].value, \n",
    "                    start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=False, d=None, D=1, \n",
    "                    trace=False, error_action='ignore',suppress_warnings=True, stepwise=True)\n",
    "    if result._is_seasonal():\n",
    "        model = {'Country': country, 'model': 'SARIMAX', 'order': result.order,\n",
    "                                     'seasonal_order': result.seasonal_order}\n",
    "    else:\n",
    "        model = {'Country': country, 'model': 'ARIMA', 'order': result.order, \n",
    "                                     'seasonal_order': None}\n",
    "        \n",
    "    models.append(model)\n",
    "    \n",
    "# convert models list into DataFrame for easy reading\n",
    "model_df = pd.DataFrame(models, columns=['zipcode', 'model', 'order', 'seasonal_order'])\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After having the model, we split data of this country into train/test dataset to evaluate the accurateness of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_country_data = df[df['Country']==country]\n",
    "second_country_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tail data of 3 year - test data set\n",
    "second_country_data.tail(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test data\n",
    "#train_df = first_country_data.iloc[:train_days] #\n",
    "train_df2 = first_country_data.loc[:split_date]\n",
    "#test_df = first_country_data.iloc[split_date:]  #\n",
    "test_start_date = split_date + timedelta(days=1) \n",
    "test_df2 = first_country_data.loc[test_start_date:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best model with full historical data for one test zipcode\n",
    "arima_model = sm.tsa.statespace.SARIMAX(train_df2['value'], \n",
    "                                        enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "# fit the model and print results\n",
    "fitted_model = arima_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the diagnostics of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model diagnostics\n",
    "fitted_model.plot_diagnostics(figsize=(16, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the prediction and the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast the test data\n",
    "forecast_values = fitted_model.predict(start=test_start_date, end=end_date, \n",
    "                                       typ='levels', dynamic=True).rename('predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot historical and forecasted data\n",
    "train_df2['train'] = train_df2['value']\n",
    "train_df2['train'].plot(legend=True,figsize=(12,6))\n",
    "test_df2['test'] = test_df2['value']\n",
    "test_df2['test'].plot(legend=True)\n",
    "forecast_values.plot(legend=True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate prediction error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tools.eval_measures import rmse\n",
    "\n",
    "error = rmse(test_df2['value'], forecast_values)\n",
    "print(f'RMSE: {error}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ROI, Profit\n",
    "date_range = pd.date_range(start_date, periods=36, freq='MS')\n",
    "forecast = pd.DataFrame(forecast_values, index=date_range[:])\n",
    "value = train_df.iloc[-1]['value']\n",
    "value_after_1_year = round(forecast.iloc[1*12-1]['predict'], 0)\n",
    "value_after_2_year = round(forecast.iloc[2*12-1]['predict'], 0)\n",
    "value_after_3_year = round(forecast.iloc[3*12-1]['predict'], 0)\n",
    "forecast_data = {'Zipcode': zipcode, 'Current Value': value, \n",
    "                 'Value After 1 Year': value_after_1_year,\n",
    "                 'Value After 2 Year': value_after_2_year,\n",
    "                 'Value After 3 Year': value_after_3_year,\n",
    "                 'Profit After 1 Year': value_after_1_year - value,\n",
    "                 'Profit After 2 Year': value_after_2_year - value,\n",
    "                 'Profit After 3 Year': value_after_3_year - value,\n",
    "                 'ROI After 1 Year': round((value_after_1_year - value) / value, 2),\n",
    "                 'ROI After 2 Year': round((value_after_2_year - value) / value, 2),\n",
    "                 'ROI After 3 Year': round((value_after_3_year - value) / value, 2)}\n",
    "forecast_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Modelling and Forecasting The Future for All Zipcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we rewrite all the code for finding best models, fitting models and forecasting into reusable functions to perform forecasting for all the chosen zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find best fit model\n",
    "def find_best_fit_models(dataframe):\n",
    "    \"\"\"\n",
    "    This function use auto_arima to find the best fit model for each zipcode\n",
    "    provided in the ``zipcodes`` list.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------\n",
    "    dataframe   : the house value time series DataFrame with columns: ['zipcode', 'value']\n",
    "    \n",
    "    Return:\n",
    "    ------------\n",
    "    A DataFrame with following data columns: ['zipcode', 'model', 'order', 'seasonal_order']\n",
    "    \"\"\"\n",
    "    # list to store best fit models\n",
    "    models = []\n",
    "    \n",
    "    # zipcodes\n",
    "    zipcodes = dataframe['zipcode'].unique()\n",
    "    \n",
    "    # loop through all zipcodes to find best model for each zipcode\n",
    "    for zipcode in zipcodes:\n",
    "        # call auto_arima to find best model\n",
    "        result = auto_arima(dataframe.loc[dataframe['zipcode'] == zipcode].value, \n",
    "                            start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=True, d=None, D=1, \n",
    "                            trace=False, error_action='ignore',suppress_warnings=True, stepwise=True)\n",
    "        \n",
    "        # build a model dictionary and put into the returned list\n",
    "        model = {'zipcode': zipcode, 'model': 'SARIMAX', 'order': result.order, 'seasonal_order': result.seasonal_order}\n",
    "        models.append(model)\n",
    "\n",
    "    # convert models list into DataFrame for easy reading\n",
    "    model_df = pd.DataFrame(models, columns=['zipcode', 'model', 'order', 'seasonal_order'])\n",
    "    \n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fit model and forecast future data\n",
    "def fit_and_forecast(dataframe, model_df):\n",
    "    \"\"\"\n",
    "    This function fit each model in ``model_df`` with the data from ``dataframe``.\n",
    "    Then, use the fitted model to forecast into the future and calculate the forecasted ROI.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframe   : the house value time series DataFrame with columns: ['zipcode', 'value']\n",
    "    model_df    : the model DataFrame output from function ``find_best_fit_models``\n",
    "    \n",
    "    Return:\n",
    "    -----------\n",
    "    A DataFrame with forecasted data,\n",
    "    And a DataFrame with forecasted ROI data\n",
    "    \"\"\"\n",
    "    \n",
    "    # forecast data to be returned\n",
    "    ROIs = []\n",
    "    forecasts = []\n",
    "    \n",
    "    # loop through all model in model_df\n",
    "    for index, model in model_df.iterrows():                \n",
    "        # fit the best model with full historical data for one test zipcode\n",
    "        data = dataframe[dataframe['zipcode']==model['zipcode']]\n",
    "        arima_model = sm.tsa.statespace.SARIMAX(data['value'], order=model['order'], \n",
    "                                                seasonal_order=model['seasonal_order'], \n",
    "                                                enforce_stationarity=False, enforce_invertibility=False)\n",
    "\n",
    "        # fit the model and print results\n",
    "        fitted_model = arima_model.fit()\n",
    "    \n",
    "        # last historical data point\n",
    "        current_data = data.tail(1)\n",
    "        current_data.reset_index(inplace=True)\n",
    "        time = current_data.iloc[0]['time']\n",
    "        zipcode = current_data.iloc[0]['zipcode']\n",
    "        value = current_data.iloc[0]['value']\n",
    "        \n",
    "        # build forecast date range\n",
    "        date_range = pd.date_range(time, periods=37, freq='MS') # first date is present\n",
    "        date_range = date_range[1:] # remove the first date\n",
    "        \n",
    "        # forecast values of the future\n",
    "        forecast_values = fitted_model.predict(start=date_range[0],\n",
    "                  end=date_range[-1], typ='levels').rename('value')\n",
    "        \n",
    "        # build forecast dataframe\n",
    "        forecast_df = pd.DataFrame(forecast_values, index=date_range[:])\n",
    "        forecast_df['zipcode'] = zipcode\n",
    "        forecasts.append(forecast_df)\n",
    "        \n",
    "        # calculate and build forecasted ROI dataframe\n",
    "        value_after_1_year = round(forecast_df.iloc[1*12-1]['value'], 0)\n",
    "        value_after_2_year = round(forecast_df.iloc[2*12-1]['value'], 0)\n",
    "        value_after_3_year = round(forecast_df.iloc[3*12-1]['value'], 0)\n",
    "        roi_data = {'Zipcode': zipcode, \n",
    "                         'Current Value': value, \n",
    "                         'Value After 1 Year':  value_after_1_year,\n",
    "                         'Value After 2 Year':  value_after_2_year,\n",
    "                         'Value After 3 Year':  value_after_3_year,\n",
    "                         'Profit After 1 Year': value_after_1_year - value,\n",
    "                         'Profit After 2 Year': value_after_2_year - value,\n",
    "                         'Profit After 3 Year': value_after_3_year - value,\n",
    "                         'ROI After 1 Year': round((value_after_1_year - value) / value, 2),\n",
    "                         'ROI After 2 Year': round((value_after_2_year - value) / value, 2),\n",
    "                         'ROI After 3 Year': round((value_after_3_year - value) / value, 2)}\n",
    "        \n",
    "        ROIs.append(roi_data)\n",
    "    \n",
    "    # convert models list into DataFrame for easy reading\n",
    "    roi_df = pd.DataFrame(ROIs, columns=['Zipcode', 'Current Value', \n",
    "                           'Value After 1 Year', 'Value After 2 Year', 'Value After 3 Year',\n",
    "                                'Profit After 1 Year', 'Profit After 2 Year', 'Profit After 3 Year',\n",
    "                                'ROI After 1 Year', 'ROI After 2 Year', 'ROI After 3 Year'])\n",
    "    \n",
    "    # merge all forecasts into one DataFrame\n",
    "    forecast_df = pd.concat(forecasts)\n",
    "    \n",
    "    return forecast_df, roi_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare in 3 bar graphs, top30 of current_cases, top30 of current_deaths, top30 of current_recovered.\n",
    "##### Each graph has 3 color bars show data of 3 current information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 8: USE ARIMA TO PREDICT TIME SERIES DATA\n",
    "#### Since we don't have enough data to check if it has seasonal trend  therefore we don't have to check seasonal trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 9: DEEP LEARNING - CONVOLUTIONAL NEURAL NETWORK (CNN) - TO PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 10: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
